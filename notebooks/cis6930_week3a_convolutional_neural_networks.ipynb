{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cis6930-week3-convolutional-neural-networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRgpqL0KVO43"
      },
      "source": [
        "# CIS6930 Week 3: Convolutional Neural Networks\n",
        "\n",
        "---\n",
        "\n",
        "Preparation: Go to `Runtime > Change runtime type` and choose `GPU` for the hardware accelerator.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uayE3ioc3k_h"
      },
      "source": [
        "gpu_info = !nvidia-smi -L\n",
        "gpu_info = \"\\n\".join(gpu_info)\n",
        "if gpu_info.find(\"failed\") >= 0:\n",
        "  print(\"Not connected to a GPU\")\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTahfFKhDOjM"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7h-6LpjVDHW"
      },
      "source": [
        "import copy\n",
        "import random\n",
        "from time import time\n",
        "from typing import Any, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HpT2i9jDSRu"
      },
      "source": [
        "### Loading the MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9sBXgBsV6dI"
      },
      "source": [
        "import torchvision\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(\"./data\", train=True, download=True,\n",
        "                                            transform=torchvision.transforms.Compose(\n",
        "                                                [torchvision.transforms.ToTensor(),\n",
        "                                                 # For standardization: 0.1307 (mean), 0.3081 (var)\n",
        "                                                 torchvision.transforms.Normalize((0.1307,), (0.3081,))]))\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, [50000, 10000],\n",
        "                                                             generator=torch.Generator().manual_seed(5))\n",
        "test_dataset = torchvision.datasets.MNIST(\"/data\", train=False, download=True,\n",
        "                                           transform=torchvision.transforms.Compose(\n",
        "                                               [torchvision.transforms.ToTensor(),\n",
        "                                                torchvision.transforms.Normalize((0.1307,), (0.3081,))]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcnUXJ-uWC5f"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.imshow(train_dataset[0][0].squeeze(0), cmap=\"gray\", interpolation=\"none\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USLHAQqBlrLi"
      },
      "source": [
        "## Implementing Convolutional Filter & Pooling from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDsL4wQeWF4a"
      },
      "source": [
        "# Convolutional filter\n",
        "filter = np.array([[-1, -1, -1],\n",
        "                   [1, 8, 1],\n",
        "                   [-1, -1, -1]])\n",
        "\n",
        "data = train_dataset[0][0].squeeze(0).detach().cpu().numpy()\n",
        "stride = 1\n",
        "\n",
        "# \n",
        "feature_map = np.zeros((\n",
        "    int(((data.shape[0] - filter.shape[0]) / stride) + 1),\n",
        "    int(((data.shape[1] - filter.shape[1]) / stride) + 1)))\n",
        "\n",
        "for i in range(0, data.shape[0] - filter.shape[0], stride):\n",
        "    for j in range(0, data.shape[1] - filter.shape[1], stride):\n",
        "        val = 0.\n",
        "        for k in range(filter.shape[0]):\n",
        "            for l in range(filter.shape[1]):\n",
        "              # Aggregating weighted values\n",
        "              val += data[i + k][j + l] * filter[k][l]\n",
        "        feature_map[i][j] = val\n",
        "\n",
        "ax1 = plt.subplot(1, 2, 1)\n",
        "ax1.imshow(data, cmap=\"gray\", interpolation=\"none\")\n",
        "ax2 = plt.subplot(1, 2, 2)\n",
        "ax2.imshow(feature_map, cmap=\"gray\", interpolation=\"none\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQiU3gfMckTF"
      },
      "source": [
        "def pooling(data, pool_size, pooling_func):\n",
        "    \"\"\"\n",
        "    Feature map shape: ((W - F) / S) + 1\n",
        "    \"\"\"\n",
        "    pool_size_x = pool_size_y = sride = pool_size\n",
        "\n",
        "    feature_map = np.zeros((\n",
        "        int(((data.shape[0] - pool_size_x) / stride) + 1),\n",
        "        int(((data.shape[1] - pool_size_y) / stride) + 1)))\n",
        "\n",
        "    for i in range(0, data.shape[0] - pool_size_x + 1, stride):\n",
        "        for j in range(0, data.shape[1] - pool_size_y + 1, stride):\n",
        "            if pooling_func == \"max\":\n",
        "                feature_map[int(i/stride)][int(j/stride)] = data[i:i+pool_size_x, j:j+pool_size_y].max()\n",
        "            elif pooling_func == \"mean\":\n",
        "                feature_map[int(i/stride)][int(j/stride)] = data[i:i+pool_size_x, j:j+pool_size_y].mean()\n",
        "            else:\n",
        "                raise ValueError(\"Invalid pooling name\")\n",
        "    return feature_map\n",
        "\n",
        "# Pooling\n",
        "data = train_dataset[0][0].squeeze(0).detach().cpu().numpy()\n",
        "\n",
        "plt.imshow(data, cmap=\"gray\", interpolation=\"none\")\n",
        "plt.show()\n",
        "plt.imshow(pooling(data, pool_size=2, pooling_func=\"max\"), cmap=\"gray\", interpolation=\"none\")\n",
        "plt.show()\n",
        "plt.imshow(pooling(data, pool_size=2, pooling_func=\"mean\"), cmap=\"gray\", interpolation=\"none\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCSAf4Lr_A7t"
      },
      "source": [
        "## Getting Familiar with Convolution & Pooling (Hands-on Session)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wKdLuhpmFtk"
      },
      "source": [
        "### Playing & Customizing Conv Filter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95Hi2qremyba"
      },
      "source": [
        "# Sample a batch\n",
        "batch_size = 8\n",
        "dl_train = DataLoader(train_dataset, batch_size=batch_size)\n",
        "batch = next(iter(dl_train))\n",
        "X, y = batch\n",
        "\n",
        "print(X.shape) # (8, 1, 28, 28)\n",
        "\n",
        "# Set custom weight for analysis\n",
        "custom_filter1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3,\n",
        "                           stride=1, padding=1)\n",
        "print(custom_filter1.weight.shape)\n",
        "print(custom_filter1.weight)\n",
        "\n",
        "# Custom convolutional filter\n",
        "custom_filter1.weight = torch.nn.Parameter(\n",
        "    torch.FloatTensor([[\n",
        "                        [[-1, -1, -1],\n",
        "                         [8, 8, 8],\n",
        "                         [-1, -1, -1]]]]))\n",
        "print(custom_filter1.weight)\n",
        "\n",
        "# Original image\n",
        "plt.imshow(X[0].squeeze(0).detach().cpu().numpy(), cmap=\"gray\", interpolation=\"none\")\n",
        "plt.show()\n",
        "\n",
        "# Feature map\n",
        "custom_fm1 = custom_filter1(X)  # (8, 1, 28, 28)\n",
        "plt.imshow(custom_fm1[0].squeeze(0).detach().cpu().numpy(), cmap=\"gray\", interpolation=\"none\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psUAECbjmPd_"
      },
      "source": [
        "### Layer 1: Conv Filter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3MCjq4zcJoU"
      },
      "source": [
        "filter1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3,\n",
        "                    stride=1, padding=1)\n",
        "\n",
        "print(train_dataset[0][0].shape) # (n_channels, height, weight)\n",
        "\n",
        "dl_train = DataLoader(train_dataset, batch_size=batch_size)\n",
        "batch = next(iter(dl_train))\n",
        "X, y = batch\n",
        "print(X.shape)  # (batch_size, n_channels, height, weight) = (8, 1, 28, 28)\n",
        "plt.imshow(X[0].squeeze(0), cmap=\"gray\", interpolation=\"none\")\n",
        "plt.show()\n",
        "\n",
        "fm1 = filter1(X)  # (8, 1, 28, 28)\n",
        "print(fm1.shape)\n",
        "\n",
        "for n in range(8):\n",
        "    ax1 = plt.subplot(1, 2, 1)\n",
        "    ax1.imshow(X[n, 0, :, :].detach().cpu().numpy(),  cmap=\"gray\", interpolation=\"none\")\n",
        "    ax2 = plt.subplot(1, 2, 2)\n",
        "    ax2.imshow(fm1[n, 0, :, :].detach().cpu().numpy(),  cmap=\"gray\", interpolation=\"none\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHCtfMsimTD8"
      },
      "source": [
        "### Layer 2: Conv Filter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iugdoAN9huAR"
      },
      "source": [
        "filter2 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=5,\n",
        "                    stride=1, padding=1)\n",
        "fm2 = filter2(fm1)  # (8, 1, 28, 28) -> (8, 5, 26, 26)\n",
        "print(fm2.shape)\n",
        "\n",
        "for n in range(batch_size):\n",
        "    ax1 = plt.subplot(1, 2, 1)\n",
        "    ax1.imshow(fm1[n, 0, :, :].detach().cpu().numpy(),  cmap=\"gray\", interpolation=\"none\")\n",
        "    ax2 = plt.subplot(1, 2, 2)\n",
        "    ax2.imshow(fm2[n, 0, :, :].detach().cpu().numpy(),  cmap=\"gray\", interpolation=\"none\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6bP0dbwmXuq"
      },
      "source": [
        "### Layer 3: Mean Pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1eLNf3Pkp8l"
      },
      "source": [
        "filter3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "fm3 = filter3(fm2)\n",
        "print(fm3.shape)  # (8, 5, 13, 13)\n",
        "\n",
        "for n in range(batch_size):\n",
        "    for i in range(5):\n",
        "        ax = plt.subplot(1, 5, i + 1)\n",
        "        ax.imshow(fm3[n, i, :, :].detach().cpu().numpy(),  cmap=\"gray\", interpolation=\"none\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QJQ8n3TIWmf"
      },
      "source": [
        "### Flattening Feature Maps & Unflattening vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBT0PQ1hHyfa"
      },
      "source": [
        "# Make feature maps ready for Fully-connected layers!\n",
        "# (8, 5, 13, 13) -> (8, 5*13*13)\n",
        "out1 = torch.flatten(fm3, 1)\n",
        "print(out1.shape)\n",
        "\n",
        "hidden1 = nn.Linear(5*13*13, 64)\n",
        "output1 = nn.Linear(64, 10)\n",
        "\n",
        "out2 = hidden1(out1)\n",
        "out3 = output1(out2)\n",
        "print(out3.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQQg_Cr8LOef"
      },
      "source": [
        "torch.flatten(fm3, 2).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh3JdCpRk-jC"
      },
      "source": [
        "# Unflatten\n",
        "torch.flatten(fm3, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcGbvA3ElchP"
      },
      "source": [
        "nn.Unflatten(1, (5, 13, 13))(torch.flatten(fm3, 1)).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfSa5Y82BjNa"
      },
      "source": [
        "filter4 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3,\n",
        "                    stride=1, padding=1)\n",
        "fm4 = filter4(fm3)\n",
        "fm4.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHIF2EdIBb6m"
      },
      "source": [
        "## In-class Exercise 1: Make a pipeline for multiple convolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGUDqclVBbpv"
      },
      "source": [
        "pipeline = nn.Sequential(\n",
        "    # (8, 1, 28, 28) -> (8, 5, 28, 28)\n",
        "    nn.Conv2d(in_channels=1,\n",
        "              out_channels=5,\n",
        "              kernel_size=3,\n",
        "              stride=1, padding=1),\n",
        "    # (8, 5, 28, 28) -> (8, 5, 14, 14)\n",
        "    nn.AvgPool2d(kernel_size=2),\n",
        "    # <== ADD 1 LINE FOR POOLING (kernel_size=?) #####################\n",
        "    # (8, 5, 14, 14) -> (8, 5, 15, 15)\n",
        "    nn.Conv2d(in_channels=5, out_channels=5, kernel_size=2,\n",
        "              stride=1, padding=1))\n",
        "\n",
        "out = pipeline(X)\n",
        "print(out.shape)\n",
        "for n in range(batch_size):\n",
        "    for i in range(5):\n",
        "        ax = plt.subplot(1, 5, i + 1)\n",
        "        ax.imshow(out[n, i, :, :].detach().cpu().numpy(),  cmap=\"gray\", interpolation=\"none\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXDEWZVwMTd_"
      },
      "source": [
        "filter3 = nn.Conv2d(in_channels=1,\n",
        "             out_channels=5,\n",
        "             kernel_size=3,\n",
        "             stride=1, padding=1)\n",
        "filter3(X).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1jVsUdDEYxC"
      },
      "source": [
        "## In-class Exercise 2: Make 1x2x2 feature map by applying at least 2 convolution and 2 pooling filters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Bl7QiLuEOMz"
      },
      "source": [
        "#\n",
        "print(X.shape) #  (8, 1, 28, 28)\n",
        "\n",
        "pipeline = nn.Sequential(\n",
        "    # (8, 1, 28, 28) -> (8, 5, 28, 28)\n",
        "    nn.Conv2d(in_channels=1,\n",
        "              out_channels=5,\n",
        "              kernel_size=3,\n",
        "              stride=1,\n",
        "              padding=1),\n",
        "    # (8, 5, 28, 28) -> (8, 5, 7, 7)\n",
        "    nn.AvgPool2d(kernel_size=4),\n",
        "    # (8, 5, 7, 7) -> (8, 1, 8, 8)\n",
        "    nn.Conv2d(in_channels=5,\n",
        "              out_channels=1,\n",
        "              kernel_size=2,\n",
        "              stride=1, padding=1),\n",
        "    nn.AvgPool2d(kernel_size=4))\n",
        "\n",
        "out = pipeline(X)\n",
        "for n in range(batch_size):\n",
        "    ax = plt.subplot(batch_size, 1, i + 1)\n",
        "    ax.imshow(out[n, 0, :, :].detach().cpu().numpy(),  cmap=\"gray\", interpolation=\"none\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY57aHugFx6V"
      },
      "source": [
        "## Convolutional Autoencoder (Demo)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKxSMSaCRxOv"
      },
      "source": [
        "class ConvAutoEncoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 hidden_dim: int = 16):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(nn.Conv2d(in_channels=1,\n",
        "                                               out_channels=8,\n",
        "                                               kernel_size=3,\n",
        "                                               stride=1,\n",
        "                                               padding=1), #-> (_, 8, 28, 28)\n",
        "                                     nn.ReLU(),\n",
        "                                     nn.Conv2d(in_channels=8,\n",
        "                                               out_channels=4,\n",
        "                                               kernel_size=3,\n",
        "                                               stride=1,\n",
        "                                               padding=1), #-> (_, 4, 28, 28)\n",
        "                                     nn.MaxPool2d(kernel_size=2, stride=2)) #-> (_, 1, 28, 28)\n",
        "        self.flatten = nn.Flatten(start_dim=1) #-> (-, 1*28*28)\n",
        "        self.hidden1 = nn.Linear(1*28*28, hidden_dim)  # -> 16\n",
        "        self.hidden2 = nn.Linear(hidden_dim, 1*28*28)  # -> 4*28*28\n",
        "        self.unflatten = nn.Unflatten(1, (1, 28, 28))  # -> (1, 28, 28)\n",
        "        self.decoder = nn.Sequential(nn.ConvTranspose2d(in_channels=1,\n",
        "                                                        out_channels=4,\n",
        "                                                        kernel_size=3,\n",
        "                                                        stride=1,\n",
        "                                                        padding=1),  #->(_, 4, 28, 28)\n",
        "                                     nn.ReLU(),\n",
        "                                     nn.ConvTranspose2d(in_channels=4,  #->(_, 1, 28, 28)\n",
        "                                                        out_channels=1,\n",
        "                                                        kernel_size=3,\n",
        "                                                        stride=1,\n",
        "                                                        padding=1))\n",
        "        \n",
        "    def encode(self, x):\n",
        "        out = self.encoder(x)\n",
        "        out = self.flatten(out)\n",
        "        out = self.hidden1(out)\n",
        "        return out\n",
        "\n",
        "    def decode(self, x):\n",
        "        out = self.hidden2(x)\n",
        "        out = self.unflatten(out)\n",
        "        out = self.decoder(out)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.encode(x)\n",
        "        out = self.decode(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Evp_nuY5nWFK"
      },
      "source": [
        "def train(model: nn.Module,\n",
        "              train_dataset: Dataset,\n",
        "              valid_dataset: Dataset,\n",
        "              config: Dict[str, Any],\n",
        "              random_seed: int = 0):\n",
        "  \n",
        "    # Random Seeds ===============\n",
        "    torch.manual_seed(random_seed)\n",
        "    random.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "    # Random Seeds ===============\n",
        "\n",
        "    # GPU configuration\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #device = torch.device(\"tpu\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    dl_train = DataLoader(train_dataset,\n",
        "                          batch_size=config[\"batch_size\"])\n",
        "    dl_valid = DataLoader(valid_dataset)\n",
        "                  \n",
        "    # Model, Optimzier, Loss function\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = config[\"optimizer_cls\"](model.parameters(), lr=config[\"lr\"])\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    # For each epoch\n",
        "    eval_list = []\n",
        "    t0 = time()\n",
        "    best_val = None\n",
        "    best_model = None\n",
        "    for n in range(config[\"n_epochs\"]):\n",
        "        t1 = time()\n",
        "        print(\"Epoch {}\".format(n))\n",
        "        # Training\n",
        "        train_loss = 0.\n",
        "        train_pred_list = []\n",
        "        train_true_list = []\n",
        "        model.train()  # Switch to the training mode\n",
        "\n",
        "        # For each batch\n",
        "        for batch in dl_train:\n",
        "            optimizer.zero_grad()              # Initialize gradient information\n",
        "            X, y = batch\n",
        "            # X = X.view(X.size(0), -1).to(device) # (batch_size, 1, 28, 28) -> (batch_size, 768)\n",
        "            X = X.to(device)  # Keep (batch_size, 1, 28, 28)\n",
        "            out = model(X)                     # Call `forward()` function of the model\n",
        "            loss = loss_fn(torch.flatten(out, 1), torch.flatten(X, 1)) # Calculate loss \n",
        "            loss.backward()                    # Backpropagate the loss value\n",
        "            optimizer.step()                   # Update the parameters\n",
        "            train_loss += loss.data.item() * config[\"batch_size\"]\n",
        "\n",
        "        train_loss /= (len(dl_train) * config[\"batch_size\"])\n",
        "        print(\"    Training loss: {:.4f}\".format(train_loss))\n",
        "\n",
        "        # Validation\n",
        "        valid_loss = 0.\n",
        "        valid_pred_list = []\n",
        "        valid_true_list = []\n",
        "\n",
        "        model.eval()  # Switch to the evaluation mode\n",
        "        valid_emb_list = []\n",
        "        valid_label_list = []\n",
        "        for i, batch in enumerate(dl_valid):\n",
        "            X, y = batch\n",
        "            # X = X.view(X.size(0), -1).to(device) # (batch_size, 1, 28, 28) -> (batch_size, 768)\n",
        "            X = X.to(device)  # Keep (batch_size, 1, 28, 28)\n",
        "            out = model(X)\n",
        "            loss = loss_fn(torch.flatten(out, 1),\n",
        "                           torch.flatten(X, 1))\n",
        "            valid_loss += loss.data.item()\n",
        "\n",
        "        valid_loss /= len(dl_valid)\n",
        "        print(\"  Validation loss: {:.4f}\".format(valid_loss))\n",
        "\n",
        "        # Model selection\n",
        "        if best_val is None or valid_loss < best_val:\n",
        "            best_model = copy.deepcopy(model)\n",
        "            best_val = valid_loss\n",
        "\n",
        "        # Orig/generated image pair\n",
        "        x_pair = torch.cat([X[0].reshape(28, 28), torch.zeros(28, 1).to(device),\n",
        "                            model(X[0].unsqueeze(0)).reshape(28, 28)], axis=1)\n",
        "        plt.imshow(x_pair.detach().cpu().numpy(),\n",
        "                  cmap=\"gray\", interpolation=\"none\")\n",
        "        plt.show()\n",
        "\n",
        "        t2 = time()\n",
        "        print(\"     Elapsed time: {:.1f} [sec]\".format(t2 - t1))\n",
        "\n",
        "        # Store train/validation loss values\n",
        "        eval_list.append([n, train_loss, valid_loss, t2 - t1])\n",
        "\n",
        "    eval_df = pd.DataFrame(eval_list, columns=[\"epoch\", \"train_loss\", \"valid_loss\", \"time\"])\n",
        "    eval_df.set_index(\"epoch\")\n",
        "\n",
        "    print(\"Total time: {:.1f} [sec]\".format(t2 - t0))\n",
        "\n",
        "    # Return the best model and trainining/validation information\n",
        "    return {\"model\": best_model,\n",
        "            \"best_val\": best_val,\n",
        "            \"eval_df\": eval_df}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAEUJZkxnuLI"
      },
      "source": [
        "# lr = {0.01, 0.001, 0.0001}\n",
        "config = {\"optimizer_cls\": optim.Adam,\n",
        "          \"lr\": 0.0001,\n",
        "          \"batch_size\": 16,\n",
        "          \"n_epochs\": 10}\n",
        "model = ConvAutoEncoder()      # lr = 0.0001\n",
        "output = train(model, train_dataset, valid_dataset, config)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
